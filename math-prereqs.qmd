# Mathematics

{{< include macros.qmd >}}

These lecture notes use:

- algebra
- precalculus
- univariate calculus
- linear algebra
- vector calculus

Some key results are listed here.

{{< include algebra.qmd >}}

## Derivatives

:::{#thm-deriv-polynomial}
#### Derivatives of polynomials

$$\deriv{x}x^q = qx^{q-1}$$

:::

---

:::{#thm-deriv-log}

#### derivative of natural logarithm
$$\dlogf{x} = \frac{1}{x} = x^{-1}$$
:::

:::{#thm-deriv-exp}
#### derivative of exponential
$$\dexpf{x} = \expf{x}$$
:::

---

:::{#thm-product-rule}
#### Product rule
$$(ab)' = ab' + ba'$$
:::

:::{#thm-quotient-rule}
#### Quotient rule
$$(a/b)' = 1/b - (a/b^2)b'$$ 
:::

:::{#thm-chain-rule}
#### Chain rule
$$\derivf{a}{c} = \derivf{a}{b} \derivf{b}{c}$$
:::

---

:::{#cor-deriv-log-chain}

#### Chain rule for logarithms
$$
\deriv{x}\log{f(x)} = \frac{f'(x)}{f(x)}
$$

:::

::: proof
Apply @thm-chain-rule and @thm-deriv-log.
:::

---

## Vector Calculus {#sec-vector-calculus}

(adapted from @fieller2018basics, ยง7.2)

Let $\vx$ and $\vb$ be vectors of length $p$, 
or in other words, matrices of length $p \times 1$:

$$
\vx = \begin{bmatrix}
x_{1} \\
x_{2} \\
\vdots \\
x_{p}
\end{bmatrix} \\
$$

$$
\vb = \begin{bmatrix}
\beta_{1} \\
\beta_{2} \\
\vdots \\
\beta_{p}
\end{bmatrix}
$$

:::{#def-transpose}
#### Transpose

The transpose of a row vector is the column vector with the same 
sequence of entries:

$$
\vx' \equiv \vx^\top \equiv [x_1, x_2, ..., x_p]
$$

:::

:::{#exm-dot-product}
#### Dot product as matrix multiplication

$$
\vx\'\vb
= [x_1, x_2, ..., x_p] 
\begin{bmatrix}
\beta_{1} \\
\beta_{2} \\
\vdots \\
\beta_{p}
\end{bmatrix} = 
x_1\beta_1+x_2\beta_2 +...+x_p \beta_p 
$$
:::

:::{#thm-transpose-sum}
#### Transpose of a sum
$$(\vx+\vy)\' = \vx\' + \vy\'$$
:::

---

::: {#def-vector-derivative}
#### Vector derivative of a vector-to-scalar function
If $f(\vb)$ is a function that takes a vector $\vb$ as input and outputs a scalar, 
such as $f(\vb) = x'\vb$, 
then:

$$
\deriv{ \vb} f(\vb) = 
\begin{bmatrix}
\deriv{\beta_1}f(\vb) \\
\deriv{\beta_2}f(\vb) \\
\vdots \\
\deriv{\beta_p}f(\vb)
\end{bmatrix}
$$
:::

---

:::{#thm-deriv-lincom}

#### Derivative of a linear combination

$$
\deriv{\vb} \vx\' \vb = x
$$

:::: notes
This looks a lot like non-vector calculus, except that you have to transpose the coefficient.
::::
:::

---

::: proof

$$ 
\ba
\deriv{ \beta} (x\'\beta) 
&= 
\begin{bmatrix}
\deriv{\beta_1}(x_1\beta_1+x_2\beta_2 +...+x_p \beta_p ) \\
\deriv{\beta_2}(x_1\beta_1+x_2\beta_2 +...+x_p \beta_p ) \\
\vdots \\
\deriv{\beta_p}(x_1\beta_1+x_2\beta_2 +...+x_p \beta_p )
\end{bmatrix} 
\\ &=
\begin{bmatrix}
x_{1} \\
x_{2} \\
\vdots \\
x_{p}
\end{bmatrix}
\\ &= \vx
\ea
$$

:::

---

:::{#def-quadratic-form}
#### Quadratic form

A **quadratic form** is a mathematical expression with the structure

$$\vx\' \matr{S} \vx$$

where $\vx$ is a vector and $\matr{S}$ is a matrix with compatible dimensions for vector-matrix multiplication.

:::

::: notes
Quadratic forms occur frequently in regression models. They are the matrix-vector generalizations of the scalar quadratic form $cx^2 = xcx$.
:::

---

:::{#thm-quadratic-form}

#### Derivative of a quadratic form

If $S$ is a $p\times p$ matrix that is constant with respect to $\beta$, then:

$$
\deriv{\beta} \beta'S\beta = 2S\beta
$$

:::
 
::: notes
This is like taking the derivative of $cx^2$ with respect to $x$ in non-vector calculus.
:::

---

:::{#cor-deriv-normsq}

#### Derivative of a simple quadratic form

$$
\deriv{\vb} \vb'\vb = 2\vb
$$

:::

::: notes
This is like taking the derivative of $x^2$.
:::


## Additional resources

### Calculus 

- @khuri2003advanced

### Linear Algebra and Vector Calculus

- @fieller2018basics
- @banerjee2014linear
- @searle2017matrix

### Numerical Analysis

- [Hua Zhou](https://hua-zhou.github.io/)'s [lecture notes for "UCLA Biostat 216 - Mathematical Methods for Biostatistics" (2023 Fall)](https://ucla-biostat-216.github.io/2023fall/schedule/schedule.html)
